Nodes represent actions
Edge  represents flow of data

3 Key types of nodes  for creating agents using LangGraph

  LLMNode: It represents LLM which is used (Ex: GPT-4o)
  TooNode :  This node is responsible for triggering external actions
             such as calling an API or executing function
  DecsionNode: This node helps the agent decide
               the next steps based on the output from LLM or tool node

Lets create basic Q&A Agent
  Following are the key components in our Agent
    Accept user input  
    Use LLM Intelligence to  generate ie Understand the question and generate response 
    Trigger the tool nodes when needed (Ex: Fetching weather information of stock info)
    Respond back to user with detailed answer.

Generate Graph
   InputNode
   LLMNode
   ToolNode
   OutputNode

Foundations to understand first AI agent promgram
  In Python, using [-1] on a list fetches the last element.
  The .content accesses the "content" value of the last message.
  model.invoke(messages[-1].content) -> This line sends the content of the last message (likely the user's most recent query) to the LLM for processing.
  
   return {"messages": [response]}
Explanation:
After the model generates a response, it is wrapped in a dictionary with a "messages" key. 
This structure may align with the format expected by the applicationâ€™s conversation state.


Sample AI agent

from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, MessagesState, START, END
from dotenv import load_dotenv
import os

# Initialize the LLM (using OpenAI in this example)
api_key = os.getenv("OPENAI_API_KEY")
model = ChatOpenAI(model="gpt-4o-mini", api_key=api_key)

# Function to handle the user query and call the LLM
def call_llm(state: MessagesState):
    messages = state["messages"]
    response = model.invoke(messages[-1].content)
    return {"messages": [response]}

# Define the graph
workflow = StateGraph(MessagesState)

# Add the node to call the LLM
workflow.add_node("call_llm", call_llm)

# Define the edges (start -> LLM -> end)
workflow.add_edge(START, "call_llm")
workflow.add_edge("call_llm", END)

# Compile the workflow
app = workflow.compile()

# Example input message from the user
input_message = {    
    "messages": [("human", "What is the capital of India?")]
}

# Run the workflow
for chunk in app.stream(input_message, stream_mode="values"):
    chunk["messages"][-1].pretty_print()
