Nodes represent actions
Edge  represents flow of data

3 Key types of nodes  for creating agents using LangGraph

  LLMNode: It represents LLM which is used (Ex: GPT-4o)
  TooNode :  This node is responsible for triggering external actions
             such as calling an API or executing function
  DecsionNode: This node helps the agent decide
               the next steps based on the output from LLM or tool node

Lets create basic Q&A Agent
  Following are the key components in our Agent
    Accept user input  
    Use LLM Intelligence to  generate ie Understand the question and generate response 
    Trigger the tool nodes when needed (Ex: Fetching weather information of stock info)
    Respond back to user with detailed answer.

Generate Graph
   InputNode
   LLMNode
   ToolNode
   OutputNode


Sample AI agent

from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, MessagesState, START, END
from dotenv import load_dotenv
import os

# Initialize the LLM (using OpenAI in this example)
api_key = os.getenv("OPENAI_API_KEY")
model = ChatOpenAI(model="gpt-4o-mini", api_key=api_key)

# Function to handle the user query and call the LLM
def call_llm(state: MessagesState):
    messages = state["messages"]
    response = model.invoke(messages[-1].content)
    return {"messages": [response]}

# Define the graph
workflow = StateGraph(MessagesState)

# Add the node to call the LLM
workflow.add_node("call_llm", call_llm)

# Define the edges (start -> LLM -> end)
workflow.add_edge(START, "call_llm")
workflow.add_edge("call_llm", END)

# Compile the workflow
app = workflow.compile()

# Example input message from the user
input_message = {    
    "messages": [("human", "What is the capital of India?")]
}

# Run the workflow
for chunk in app.stream(input_message, stream_mode="values"):
    chunk["messages"][-1].pretty_print()


Foundations to understand first AI agent promgram

  In Python, using [-1] on a list fetches the last element.
  The .content accesses the "content" value of the last message.
  model.invoke(messages[-1].content) -> This line sends the content of the last message (likely the user's most recent query) to the LLM for processing.
  
   return {"messages": [response]}
Explanation:
After the model generates a response, it is wrapped in a dictionary with a "messages" key. 
This structure may align with the format expected by the applicationâ€™s conversation state.


app.stream(input_message, stream_mode="values"):
app.stream(...) is a method in Langgraph used to process a stream of data, typically involving user inputs and the responses generated by the AI model.
input_message is the message or query being passed as input to the stream.
stream_mode="values" specifies how the stream operates. In this case, it indicates that the method will yield chunks of data as values, 
such as intermediate results or updates from the AI model.


For Continuous input:

# Function to continuously take user input
def interact_with_agent():
    while True:
        user_input = input("You: ")
        if user_input.lower() in ["exit", "quit"]:
            print("Ending the conversation.")
            break
        input_message = {
            "messages": [("human", user_input)]
        }

        for chunk in app.stream(input_message, stream_mode="values"):
            chunk["messages"][-1].pretty_print()

# Start interacting with the agent
interact_with_agent()
